import torch
import torch.nn.functional as F
import json
import clip
import cv2
from torchvision.ops import roi_align
import sys
import numpy as np
from PIL import Image
from ultralytics import YOLO
from typing import List, Tuple
import tkinter as tk
from tkinter import filedialog

# Load m√¥ h√¨nh YOLOv5 (pretrained)
yolo_model = YOLO(r"D:\DocCument\DATN\test\yolov5\yolov5xu.pt")  # Ho·∫∑c yolov5m.pt, yolov5l.pt n·∫øu mu·ªën model m·∫°nh h∆°n

# Backbone feature capture for RoIAlign descriptors
_BACKBONE_LAYER_INDEX = 9  # SPPF layer index inside YOLOv5 backbone
_BACKBONE_STRIDE = int(yolo_model.model.model[-1].stride[-1].item())
_feature_map_store = {}

def _capture_backbone_feature(module, inputs, output):
    """Store latest backbone feature map for ROI extraction."""
    _feature_map_store["backbone"] = output.detach().cpu()

# Register hook once so every inference populates the shared store
yolo_model.model.model[_BACKBONE_LAYER_INDEX].register_forward_hook(_capture_backbone_feature)

# Load CLIP model
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üîπ Using device: {device}")
clip_model, preprocess = clip.load("ViT-B/32", device=device)

# Danh s√°ch t·ª´ v·ª±ng m·ªü r·ªông (c√≥ th·ªÉ t√πy ch·ªânh)
animals = [
    "cat", "dog", "bird", "horse", "cow", " sheep", "lion",
    "tiger", "elephant", "bear", "deer", "monkey", "zebra",
    "giraffe", "kangaroo", "dolphin", "shark", "snake", "turtle",
    "rabbit", "fox", "wolf", "panda", "crocodile", "peacock","person"
]
vehicles = [
    "car", "motorcycle", "bicycle", " bus", " truck", " train",
    "airplane", " helicopter", "boat", " yacht", " submarine",
    " scooter", " skateboard", "tram", " taxi", " police car",
    " ambulance", " fire truck", " forklift", " van"
]
household_items = [
    " chair", " table", " sofa", " bed", " lamp", " television",
    " laptop", " smartphone", " refrigerator", " microwave", " washing machine",
    "vacuum cleaner", " mirror", " bookshelf", " fan", " clock",
    " pillow", " blanket", " rug", " cupboard", " kettle", " toaster","key"
]
food_drinks = [
    " pizza", " hamburger", " sandwich", " hotdog", " steak", " fish",
    "bowl of soup", " plate of spaghetti", " salad", " cake", " donut",
    " ice cream", " cup of coffee", " bottle of water", " soda can",
    " glass of milk", " loaf of bread", " croissant", " chocolate bar"
]
clothing = [
    "t-shirt", " shirt", " pair of jeans", " dress", "skirt", " jacket",
    " coat", " pair of shorts", " hat", " cap", " pair of sunglasses",
    "pair of gloves", " belt", " scarf", " backpack", " handbag",
    " pair of shoes", " pair of sandals", " pair of boots"
]
electronics = [
    " smartphone", " laptop", " desktop computer", " keyboard", " mouse",
    " printer", " projector", " television", " camera", " drone",
    " game console", " tablet", " smartwatch", " microphone",
    " speaker", " headphone", " charger", " USB drive", " hard drive"
]
buildings = [
    "house", " skyscraper", " bridge", " tower", " lighthouse",
    " castle", " temple", "church", " mosque", " stadium",
    " factory", " warehouse", " hospital", "school", " shopping mall",
    " hotel", " police station", " fire station", " library", "a museum"
]
nature = [
    "mountain", " river", " lake", " ocean", " beach", " desert",
    "forest", " waterfall", " volcano", "cave", " rainbow",
    " sunset", " sunrise", " thunderstorm", "snow-covered mountain",
    " flower", " tree", " bush", " meadow", " glacier"
]
sports = [
    "soccer ball", " basketball", " baseball bat", " tennis racket",
    " golf club", " hockey stick", " snowboard", " skateboard",
    " pair of ice skates", " bicycle helmet", " football", " volleyball",
    " badminton racket", " boxing glove", " jump rope"
]
school_supplies = [
    " book", " notebook", " pen", " pencil", " eraser", " ruler",
    " calculator", " protractor", " compass", " highlighter", 
    " stapler", " pair of scissors", " glue stick", " backpack",
    " whiteboard", " blackboard", " piece of chalk", " marker",
    " set of colored pencils", "paintbrush", " watercolor palette",
    " binder", " paper clip", " sticky note", " file folder",
    " document scanner", " desk lamp", "tablet", " laptop", 
    " printer", " USB flash drive"
]

# Lo·∫°i b·ªè d·∫•u c√°ch th·ª´a v√† tr√°nh tr√πng nh√£n
label_texts = list(set([label.strip() for label in (
    animals + vehicles + household_items +
    food_drinks + clothing + electronics +
    buildings + nature + sports + school_supplies
)]))

def _consume_feature_map():
    """Retrieve and remove the cached backbone feature map."""
    feature_map = _feature_map_store.pop("backbone", None)
    if feature_map is None:
        raise RuntimeError("Backbone feature map not captured from the latest YOLO forward pass.")
    return feature_map

def _compute_resize_params(feature_map: torch.Tensor, image_shape: Tuple[int, int, int]):
    """Compute scale and padding applied during YOLO letterboxing."""
    feat_h, feat_w = feature_map.shape[-2:]
    input_h, input_w = feat_h * _BACKBONE_STRIDE, feat_w * _BACKBONE_STRIDE
    orig_h, orig_w = image_shape[:2]
    scale = min(input_h / max(orig_h, 1), input_w / max(orig_w, 1))
    new_w = int(round(orig_w * scale))
    new_h = int(round(orig_h * scale))
    pad_w = max(input_w - new_w, 0) / 2.0
    pad_h = max(input_h - new_h, 0) / 2.0
    return scale, pad_w, pad_h, feat_w, feat_h

def extract_roi_features(feature_map: torch.Tensor, boxes: List[Tuple[int, int, int, int]], image_shape: Tuple[int, int, int]):
    """Return L2-normalized ROI pooled feature vectors for each bounding box."""
    if feature_map is None or not boxes:
        return []
    scale, pad_w, pad_h, feat_w, feat_h = _compute_resize_params(feature_map, image_shape)
    device = feature_map.device
    dtype = feature_map.dtype
    aligned_boxes = []
    eps = 1e-3
    for x1, y1, x2, y2 in boxes:
        x1_s = (x1 * scale + pad_w) / _BACKBONE_STRIDE
        y1_s = (y1 * scale + pad_h) / _BACKBONE_STRIDE
        x2_s = (x2 * scale + pad_w) / _BACKBONE_STRIDE
        y2_s = (y2 * scale + pad_h) / _BACKBONE_STRIDE
        x1_s = min(max(x1_s, 0.0), feat_w - eps)
        y1_s = min(max(y1_s, 0.0), feat_h - eps)
        x2_s = min(max(x2_s, x1_s + eps), feat_w - eps)
        y2_s = min(max(y2_s, y1_s + eps), feat_h - eps)
        aligned_boxes.append([0.0, x1_s, y1_s, x2_s, y2_s])
    if not aligned_boxes:
        return []
    rois = torch.tensor(aligned_boxes, device=device, dtype=dtype)
    pooled = roi_align(feature_map, rois, output_size=(7, 7), spatial_scale=1.0, aligned=True)
    pooled = F.adaptive_avg_pool2d(pooled, (1, 1)).flatten(1)
    pooled = F.normalize(pooled, p=2, dim=1)
    return pooled.cpu().tolist()

text_inputs = clip.tokenize(label_texts).to(device)

def add_padding(image, bbox, padding=10):
    x1, y1, x2, y2 = bbox
    img_height, img_width = image.shape[:2]
    return image[max(0, y1-padding):min(y2+padding, img_height), max(0, x1-padding):min(x2+padding, img_width)]

# Detect objects with YOLO
def detect_objects(image_path):
    image = cv2.imread(image_path)
    results = yolo_model(image_path)
    feature_map = _consume_feature_map()
    detected_objects, yolo_labels = [], []

    for result in results:
        boxes = result.boxes.xyxy.cpu().numpy()
        classes = result.boxes.cls.cpu().numpy()
        for box, cls in zip(boxes, classes):
            x1, y1, x2, y2 = map(int, box)
            if (x2 - x1 < 20) or (y2 - y1 < 20):
                print(f"‚ö†Ô∏è B·ªè qua ƒë·ªëi t∆∞·ª£ng nh·ªè qu√° [{x1}, {y1}, {x2}, {y2}]")
                continue
            cropped_pil = Image.fromarray(cv2.cvtColor(add_padding(image, (x1, y1, x2, y2)), cv2.COLOR_BGR2RGB))
            detected_objects.append((cropped_pil, (x1, y1, x2, y2)))
            yolo_labels.append(results[0].names[int(cls)])
    return detected_objects, yolo_labels, image, feature_map

# Ph√¢n lo·∫°i v·ªõi CLIP, fallback v·ªÅ YOLO n·∫øu confidence th·∫•p
def classify_with_clip(detected_objects, yolo_labels):
    results = []
    text_features = clip_model.encode_text(text_inputs)

    # üåü L·∫•y to√†n b·ªô nh√£n t·ª´ YOLO l√†m nh√£n quan tr·ªçng
    important_labels = list(yolo_model.names.values())
    print(f"üîπ Danh s√°ch nh√£n quan tr·ªçng t·ª´ YOLO: {important_labels}")

    for idx, (cropped_pil, bbox) in enumerate(detected_objects):
        try:
            if not isinstance(cropped_pil, Image.Image):
                print(f"‚ö†Ô∏è ƒê·ªëi t∆∞·ª£ng {idx+1} kh√¥ng ph·∫£i ·∫£nh PIL! Gi·ªØ nh√£n YOLO: '{yolo_labels[idx]}'")
                results.append((yolo_labels[idx], bbox))
                continue

            # ƒê∆∞a ·∫£nh v√†o CLIP ƒë·ªÉ ph√¢n lo·∫°i
            image_input = preprocess(cropped_pil).unsqueeze(0).to(device)
            with torch.no_grad():
                similarities = (clip_model.encode_image(image_input) @ text_features.T).softmax(dim=-1)
                best_label = label_texts[similarities.argmax().item()]
                confidence = similarities.max().item()

            # üéØ Logic th√¥ng minh gi·ªØ nh√£n YOLO n·∫øu CLIP nh·∫≠n sai
            if yolo_labels[idx] in important_labels and best_label != yolo_labels[idx]:
                print(f"üîπ Gi·ªØ nh√£n YOLO (quan tr·ªçng): '{yolo_labels[idx]}' d√π CLIP b√°o '{best_label}' (conf: {confidence:.2f})")
                results.append((yolo_labels[idx], bbox))
            elif confidence < 0.3:
                print(f"‚ö†Ô∏è ƒê·ªô t·ª± tin th·∫•p ({confidence:.2f}) ‚Üí Gi·ªØ nh√£n YOLO: '{yolo_labels[idx]}'")
                results.append((yolo_labels[idx], bbox))
            else:
                print(f"‚úÖ ƒê·ªïi nh√£n CLIP: '{yolo_labels[idx]}' ‚ûú '{best_label}' (confidence: {confidence:.2f})")
                results.append((best_label.strip(), bbox))

        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói CLIP: {e} ‚Üí Gi·ªØ nh√£n YOLO: '{yolo_labels[idx]}'")
            results.append((yolo_labels[idx], bbox))

    return results

# Full pipeline
def run_pipeline(image_path=None):
    if image_path is None:
        root = tk.Tk()
        root.withdraw()
        image_path = filedialog.askopenfilename(title="Ch·ªçn file ·∫£nh", filetypes=[("Image files", "*.jpg *.jpeg *.png")])
        if not image_path:
            print("‚ùå Kh√¥ng c√≥ file n√†o ƒë∆∞·ª£c ch·ªçn!")
            return

    detected_objects, yolo_labels, original_image, feature_map = detect_objects(image_path)
    classified_results = classify_with_clip(detected_objects, yolo_labels)

    boxes = [bbox for _, bbox in classified_results]
    roi_features = extract_roi_features(feature_map, boxes, original_image.shape)
    if boxes:
        feature_dim = feature_map.shape[1]
        if len(roi_features) != len(boxes):
            fallback = [0.0] * feature_dim
            while len(roi_features) < len(boxes):
                roi_features.append(fallback.copy())
            if len(roi_features) > len(boxes):
                roi_features = roi_features[:len(boxes)]
    else:
        roi_features = []
    if roi_features:
        print(f"Do. Extracted {len(roi_features)} ROIAlign feature vectors (dim {len(roi_features[0]) if roi_features else 0}).")
    results_json = []
    for idx, (label, (x1, y1, x2, y2)) in enumerate(classified_results):
        cv2.rectangle(original_image, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(original_image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        feature_vector = roi_features[idx] if idx < len(roi_features) else []
        if feature_vector:
            feature_vector = [float(v) for v in feature_vector]
        results_json.append({"label": label, "bbox": [int(x1), int(y1), int(x2), int(y2)], "feature": feature_vector})
        print(f"‚úÖ ƒê·ªëi t∆∞·ª£ng {idx+1} | Class: {label} | BBox: [{x1}, {y1}, {x2}, {y2}]")

    with open("result.json", "w") as json_file:
        json.dump(results_json, json_file, indent=4)

    cv2.imwrite("result.jpg", original_image)
    print("‚úÖ Nh·∫≠n di·ªán ho√†n t·∫•t! K·∫øt qu·∫£ ƒë√£ l∆∞u.")

if __name__ == "__main__":
    image_path = sys.argv[1] if len(sys.argv) > 1 else None
    run_pipeline(image_path)
